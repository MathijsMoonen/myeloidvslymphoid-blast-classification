{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e66518e-71cb-4c3d-bccf-f85f17d8b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import  glob\n",
    "import time\n",
    "import albumentations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder# creating instance of one-hot-encoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "### Internal Imports\n",
    "from models.ResNext50 import Myresnext50\n",
    "from train.train_classification import trainer_classification\n",
    "from utils.utils import configure_optimizers\n",
    "from Datasets.DataLoader import Img_DataLoader\n",
    "\n",
    "### PyTorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe32bca7-aab5-41df-afd9-98727f30a1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (17669, 4)          Training set label count: {'Other': 10069, 'Myeloid': 6582, 'Lymphoid': 1018} \n",
      "\n",
      "Validation set shape: (3119, 4)         Validation set label count: {'Other': 1777, 'Myeloid': 1162, 'Lymphoid': 180} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data, split it into training and validation dataframes\n",
    "df = pd.read_pickle('imagepaths.pkl')\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.15, random_state=42, stratify=df['Label'])\n",
    "print(f\"Training set shape: {train_df.shape}          Training set label count: {str(Counter(train_df['Label'].to_list()))[7:][1:][:-1]} \\n\")\n",
    "print(f\"Validation set shape: {val_df.shape}         Validation set label count: {str(Counter(val_df['Label'].to_list()))[7:][1:][:-1]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ba1fe00a-3ec1-4575-b1d1-6e81abc5f2b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\moone/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
      "C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt50_32X4D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt50_32X4D_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Load filepaths\n",
    "X_train = train_df['Filepath'].to_list()\n",
    "X_val = str(val_df['Filepath'].to_list())\n",
    "\n",
    "# Load df that represents the one hot encoding of each cell type (Myeloid, Lymphoid, other)\n",
    "cell_types_df = pd.read_pickle(\"cell_types_df.pkl\")\n",
    "\n",
    "# Load model\n",
    "resnext50_pretrained = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=True)\n",
    "my_extended_model = Myresnext50(my_pretrained_model= resnext50_pretrained, num_classes = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b2860b0-f20c-49fb-b22e-18f535ebbde9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AML'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].split('_')[0].split(\"\\\\\")[-1].replace(\" \", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a3ea6b-a52c-4ac3-bd4d-de5aeab45789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple augumentation to improve the data generalibility\n",
    "\n",
    "transform_pipeline = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e317cbab-fa58-4e9e-ab1b-4d9ce018e9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training setup\n",
    "trainer = trainer_classification(train_image_files=X_train, validation_image_files=X_val, model=my_extended_model,\n",
    "                                     img_transform=transform_pipeline, init_lr=0.001,\n",
    "                                     lr_decay_every_x_epochs=10,\n",
    "\n",
    "                                     weight_decay=0.0005, batch_size=1024, epochs=30, gamma=0.1, df=cell_types_df,\n",
    "                                     save_checkpoints_dir='checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39a05e49-861e-4464-b168-f9823ac383c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Create model.\n",
      "==> List learnable parameters\n",
      "==> Load data.\n",
      "17669\n",
      "372663\n",
      "==> Configure optimizer.\n",
      "10\n",
      "==> Start training\n",
      "==> Creat the saving dictionary\n",
      "Created new dictionary for saving checkpoints\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"D:\\Mathijs\\Open Universiteit\\Thesis\\Implementation\\DeepHeme_training\\Datasets\\DataLoader.py\", line 44, in __getitem__\n    label = img_path.split('/')[-2]\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m My_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_extended_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Mathijs\\Open Universiteit\\Thesis\\Implementation\\DeepHeme_training\\train\\train_classification.py:132\u001b[0m, in \u001b[0;36mtrainer_classification.train\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    129\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_checkpoints_dir)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch):\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m     _loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_one_epoch(val_data_loader, model, epoch)\n\u001b[0;32m    134\u001b[0m     loss_list\u001b[38;5;241m.\u001b[39mappend(_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mD:\\Mathijs\\Open Universiteit\\Thesis\\Implementation\\DeepHeme_training\\train\\train_classification.py:47\u001b[0m, in \u001b[0;36mtrainer_classification.train_one_epoch\u001b[1;34m(self, epoch, train_loader, model, optimizer, lr_scheduler)\u001b[0m\n\u001b[0;32m     45\u001b[0m t0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[1;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\_utils.py:705\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\moone\\miniconda3\\envs\\thesis\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"D:\\Mathijs\\Open Universiteit\\Thesis\\Implementation\\DeepHeme_training\\Datasets\\DataLoader.py\", line 44, in __getitem__\n    label = img_path.split('/')[-2]\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "My_model = trainer.train(my_extended_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
